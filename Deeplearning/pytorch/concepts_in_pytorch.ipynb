{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Pytorch: define variables and autograd \n",
    "\n",
    "\n",
    "References: https://github.com/jcjohnson/pytorch-examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2942884.75)\n",
      "(100, 173.30369567871094)\n",
      "(200, 3.2803146839141846)\n",
      "(300, 0.11890672147274017)\n",
      "(400, 0.005051522050052881)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "N, D_in, H, D_out = 64, 100, 100, 10\n",
    "\n",
    "# create random input and output\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# intialize\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "lr = 1e-5\n",
    "T = 500\n",
    "\n",
    "for t in range(T):\n",
    "    \n",
    "    # forward\n",
    "    h=x.mm(w1)\n",
    "    h_relu=h.clamp(min=0)\n",
    "    y_pred=h_relu.mm(w2)\n",
    "    \n",
    "    # loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t%100 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    \n",
    "    #backward using autograd, computing the gradient wrt all variables requires_grad=True\n",
    "    loss.backward()\n",
    "    \n",
    "    w1.data -= lr*w1.grad.data\n",
    "    w2.data -= lr*w2.grad.data\n",
    "    \n",
    "    # zero out gradient after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models using __nn__ module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 8.97107388375673e-12)\n",
      "(100, 9.212602902763933e-12)\n",
      "(200, 8.818723529202543e-12)\n",
      "(300, 8.562300174430604e-12)\n",
      "(400, 8.671823675809875e-12)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "T = 500\n",
    "\n",
    "for t in range(T):\n",
    "    # forward\n",
    "    y_pred = model(x)\n",
    "    # loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    if t%100 == 0:\n",
    "        print(t, loss.data[0])    \n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad.data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using optim module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.001861593802459538)\n",
      "(100, 2.819776454998646e-06)\n",
      "(200, 8.927323463803205e-11)\n",
      "(300, 1.1604898465800151e-11)\n",
      "(400, 1.3015999736354367e-11)\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # forward \n",
    "    y_pred = model(x)\n",
    "\n",
    "    # loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%100 == 0:\n",
    "        print(t, loss.data[0]) \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward \n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Computational Graph\n",
    "\n",
    "Define a forward function with random number of forward passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 797.9113159179688)\n",
      "(100, 791.23046875)\n",
      "(200, 785.10205078125)\n",
      "(300, 779.4408569335938)\n",
      "(400, 774.1824340820312)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define Dynamic Neural Nets via overriding nn.Module\n",
    "class DN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DN, self).__init__()\n",
    "        self.IH = nn.Linear(D_in, H)\n",
    "        self.HH = nn.Linear(H, H)\n",
    "        self.HO = nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        h_relu = self.IH(x).clamp(min=0)\n",
    "        # random.randint(10,10)\n",
    "        for _ in range(0,5):\n",
    "            h_relu = self.HH(h_relu).clamp(min=0)\n",
    "        y_pred = self.HO(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "H = 2\n",
    "dn = DN(D_in, H, D_out)\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(dn.parameters(), lr=lr)\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # forward\n",
    "    y_pred = dn(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t%100 == 0:\n",
    "        print(t, loss.data[0])\n",
    "        \n",
    "#         p_list = dn.parameters()\n",
    "#         for p in dn.parameters():\n",
    "#             size = p.data.size()\n",
    "#             print size\n",
    "        \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
