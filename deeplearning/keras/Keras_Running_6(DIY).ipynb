{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Running 6(DIY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras prvides Earlystopping() which allows us to define where we can stop train our model. \n",
    "\n",
    "However, the parameter(**patience**) is a very important parameter. If we only use Earlystopping(), we have to train a net many times to find a good **patience**. It is a waste of time doing this. \n",
    "\n",
    "A better way is that we can first train a network for enough epoches or batches. Then we use Earlystopping() to find the best epoch or batch. In this way, we can only train the network once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the the source codes of EarlyStopping for Keras.GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(Callback):\n",
    "    '''Stop training when a monitored quantity has stopped improving.\n",
    "    # Arguments\n",
    "        monitor: quantity to be monitored.\n",
    "        patience: number of epochs with no improvement\n",
    "            after which training will be stopped.\n",
    "        verbose: verbosity mode.\n",
    "        mode: one of {auto, min, max}. In 'min' mode,\n",
    "            training will stop when the quantity\n",
    "            monitored has stopped decreasing; in 'max'\n",
    "            mode it will stop when the quantity\n",
    "            monitored has stopped increasing.\n",
    "    '''\n",
    "    def __init__(self, monitor='val_loss', patience=0, verbose=0, mode='auto'):\n",
    "        super(EarlyStopping, self).__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.patience = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('EarlyStopping mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (self.mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.wait = 0       # Allow instances to be re-used\n",
    "        self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn('Early stopping requires %s available!' %\n",
    "                          (self.monitor), RuntimeWarning)\n",
    "\n",
    "        if self.monitor_op(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: early stopping' % (epoch))\n",
    "                self.model.stop_training = True\n",
    "            self.wait += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is thatï¼š \n",
    "1. initialize;\n",
    "2. wait = 0, best = np.Inf\n",
    "3. get the monitor\n",
    "4. the main part of this algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own dxEarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dxEarlyStopping(patience, mode, monitor_log):\n",
    "    '''arguments:\n",
    "    patience: the longest iteration wait for\n",
    "    mode: 'min' or 'max'\n",
    "    monitor_log: vector of acc of loss\n",
    "    \n",
    "    output: we should stop trainning here\n",
    "    '''\n",
    "    wait = 0\n",
    "    if mode == 'min':\n",
    "            monitor_op = np.less\n",
    "        elif mode == 'max':\n",
    "            monitor_op = np.greater\n",
    "    best = np.Inf if self.monitor_op == np.less else -np.Inf\n",
    "    \n",
    "    for i in range(len(monitor_log)):\n",
    "        current = monitor_log[i]\n",
    "        if monitor_op(current, self.best):\n",
    "            best = current\n",
    "            wait = 0\n",
    "        else:\n",
    "            if wait >= patience:\n",
    "                print('Epoch %05d: early stopping' % (i))\n",
    "                beststop = i\n",
    "                break\n",
    "            self.wait += 1\n",
    "    return beststop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
