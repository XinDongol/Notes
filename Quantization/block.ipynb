{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/scaling_by_real_value.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.4 2.4 7.2 7.2]\n"
     ]
    }
   ],
   "source": [
    "def scaling_by_real(x):\n",
    "    m = 3\n",
    "    s = np.max(x) / (2**(m-1) - 1)\n",
    "    \n",
    "    res = np.rint(x / s) * s\n",
    "    return res\n",
    "\n",
    "inp = np.array([0.7, 1.4, 2.50, 6.001, 7.2])\n",
    "quant_inp = scaling_by_real(inp)\n",
    "\n",
    "print(quant_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/scaling_by_pot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 4. 8. 8.]\n"
     ]
    }
   ],
   "source": [
    "def round_to_power_of_two(n):\n",
    "    return 2**np.ceil(np.log2(n))\n",
    "\n",
    "def scaling_by_pot(x):\n",
    "    m = 3\n",
    "    s = np.max(x) / (2**(m-1) - 1)\n",
    "    s = round_to_power_of_two(s)\n",
    "    \n",
    "    res = np.rint(x / s) * s\n",
    "    return res\n",
    "\n",
    "inp = np.array([0.7, 1.4, 2.50, 6.001, 7.2])\n",
    "quant_inp = scaling_by_pot(inp)\n",
    "\n",
    "print(quant_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/scaling_by_multi_real.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [1.66666667 2.5       ] [7.2 7.2]\n"
     ]
    }
   ],
   "source": [
    "inp = np.array([0.7, 1.4, 2.50, 6.001, 7.2])\n",
    "# partition\n",
    "inp_1 = inp[[0, 3, 4]]\n",
    "inp_2 = inp[[1, 2]]\n",
    "\n",
    "quant_inp_1 = scaling_by_real(inp_1)\n",
    "quant_inp_2 = scaling_by_real(inp_2)\n",
    "\n",
    "print(quant_inp_1[[0]], quant_inp_2, quant_inp_1[[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/two_level_scaling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [1.2 2.4] [7.2 7.2]\n"
     ]
    }
   ],
   "source": [
    "inp = np.array([0.7, 1.4, 2.50, 6.001, 7.2])\n",
    "\n",
    "# first-level (real) scaling\n",
    "m = 3 # number of bit \n",
    "s = np.max(inp) / (2**(m-1) - 1)\n",
    "inp /= s\n",
    "# print(inp)\n",
    "\n",
    "# partitioning\n",
    "inp_1 = inp[[0, 3, 4]]\n",
    "inp_2 = inp[[1, 2]]\n",
    "# print(inp_1, inp_2)\n",
    "\n",
    "# second-level (PoT) scaling and quantization\n",
    "quant_inp_1 = scaling_by_pot(inp_1)\n",
    "quant_inp_2 = scaling_by_pot(inp_2)\n",
    "\n",
    "# Rescale by the first-level scaling factor\n",
    "quant_inp_1 *= s\n",
    "quant_inp_2 *= s\n",
    "\n",
    "print(quant_inp_1[[0]], quant_inp_2, quant_inp_1[[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/unification.png)\n",
    "\n",
    "The aforementioned two-level scaling framework is quite general and can explain many existing quantization methods. \n",
    "\n",
    "$s$ is the scale and $ss$ is the sub-scale. \n",
    "\n",
    "If scale (or subscale) is a power-of-two number, we can use bit shift to implement it and we call it \"Hardware Type\", otherwise, we call it \"Software Type\". \n",
    "\n",
    "$k_1$ (or $k_2$) is the block size (granularity of the scaling) of the scale (or subscale). A smaller block size means a finer granularity, which is more flexible but also more expensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MX Format\n",
    "\n",
    "We next introduce the MX format under the framework of two-level scaling. \n",
    "\n",
    "![](./imgs/three_examples.png)\n",
    "\n",
    "The MX format has the following properties:\n",
    "1. Both the first and second level scaling are power-of-two, which is convenient for hardware implementation.\n",
    "2. Both the first and second level granularity are fine-grained, which is flexible for different applications. In addition, the second level's scaling factor has only 1 bit to reduce the overhead of storing the scaling factor.\n",
    "\n",
    "We can compute the average bit-width of the MX format, \n",
    "\n",
    "![](./imgs/mx_average_bit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Implementation (MX Dot Product)\n",
    "\n",
    "![](./imgs/hw_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the length of input vectors is $r$. Each vector has $\\frac{r}{k_1}$ blocks, and thus $\\frac{r}{k_1}$ different 1st level scaling factors $s$.\n",
    "\n",
    "Dot product of two vectors consists of two operations: point-wise multiplication (also known as Hadamard product) and reduction (summing up the results of point-wise multiplication). \n",
    "\n",
    "The point-wise multiplication and reduction can be done by multiplying each block of the first vector with the corresponding block of the second vector and summing up results. Then, we can sum up the results of all blocks to get the final result of dot product. \n",
    "\n",
    "![](./imgs/dot_product.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
